{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6842a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "qdrant = QdrantClient(\n",
    "    url=\"https://995828ea-aab5-412e-9766-dd32680878b2.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
    "    api_key=qdrant_api_key,\n",
    "    check_compatibility=False,\n",
    "    timeout=180\n",
    ")\n",
    "qdrant.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE)  # BioBERT = 768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29ad00d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1122"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parent_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62ebd928",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant.upload_points(\n",
    "    collection_name =collection_name,\n",
    "    points=fifth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a2ffad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open(\"parent_ids.json\",\"w\") as F:\n",
    "    json.dump(parent_ids,F,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada89e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "766f8406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1352"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parent_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f630d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"WHO IS CRISTIANO RONALDO\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_completion_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=True,\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d42541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_8296\\3329933065.py:19: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from qdrant_client.http.models import PointStruct\n",
    "from uuid import uuid4\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "import os\n",
    "import json \n",
    "import re\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "qdrant_api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "collection_name = \"gale_books\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
    "\n",
    "\n",
    "listofBooks = [f\"gale_articles_Vol_{i}.json\" for i in range(1,6)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def MainIndexing(listOfBooks:list):\n",
    "    for book  in listofBooks:\n",
    "        with open(book,\"r\",encoding=\"utf-8\") as f :\n",
    "            content = json.load(f)\n",
    "            for i in range(len(content)):\n",
    "                article_title = content[i][\"title\"]\n",
    "                article = content[i][\"content\"]\n",
    "                # Clean the article\n",
    "                cleaned_article = CleanArticle(article)\n",
    "                #create_ParentChild Chunking \n",
    "                ParentChildChunking(cleaned_article,article_title)\n",
    "        print(f\"The article of {book} are embedded and inserted in the DB\")\n",
    "        print(\"==================================\")\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "228c1e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_13656\\729879767.py:19: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from qdrant_client.http.models import PointStruct\n",
    "from uuid import uuid4\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import time\n",
    "import os\n",
    "import json \n",
    "import re\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "qdrant_api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "collection_name = \"gale_books\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
    "\n",
    "\n",
    "listofBooks = [f\"gale_articles_Vol_{i}.json\" for i in range(1,6)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def MainIndexing(Book):\n",
    "        with open(Book,\"r\",encoding=\"utf-8\") as f :\n",
    "            content = json.load(f)\n",
    "            for i in range(len(content)):\n",
    "                article_title = content[i][\"title\"]\n",
    "                article = content[i][\"content\"]\n",
    "                # Clean the article\n",
    "                cleaned_article = CleanArticle(article)\n",
    "                #create_ParentChild Chunking \n",
    "                ParentChildChunking(cleaned_article,article_title)\n",
    "                if i%50 == 0:\n",
    "                     time.sleep(30)\n",
    "        print(f\"The article of {Book} are embedded and inserted in the DB\")\n",
    "        print(\"==================================\")\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a4a5bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_ids = {}\n",
    "def SecondAlter(book):\n",
    "    points = []\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=150,chunk_overlap= 50)\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
    "    with open(book,\"r\",encoding=\"utf-8\") as f :\n",
    "            content = json.load(f)\n",
    "            for i in range(len(content)):\n",
    "                article_title = content[i][\"title\"]\n",
    "                article = content[i][\"content\"]\n",
    "                # Clean the article\n",
    "                cleaned_article = CleanArticle(article)\n",
    "                chunks_doc = []\n",
    "                parent_id = str(uuid4())\n",
    "                parent_ids[parent_id] = cleaned_article\n",
    "                # Chunk the article \n",
    "                chunks = text_splitter.create_documents(\n",
    "                    [cleaned_article],\n",
    "                    metadatas=[{\n",
    "                        \"title\":article_title,# must be modified\n",
    "                        \"parent_id\": parent_id}]\n",
    "                )\n",
    "                chunks_doc.extend(chunks)\n",
    "                # embedd the vectors \n",
    "                vectors = embedding_model.embed_documents([c.page_content for c in chunks_doc])\n",
    "                # Create Point for Qdrant injection \n",
    "\n",
    "                for index, doc in enumerate(chunks_doc):\n",
    "                    point = PointStruct(\n",
    "                        id=str(uuid4()),\n",
    "                        vector=vectors[index],\n",
    "                        payload={\n",
    "                            **doc.metadata\n",
    "                        }\n",
    "                    )\n",
    "                    points.append(point)\n",
    "    return points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caea30b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    first = SecondAlter(\"gale_articles_Vol_2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "975f2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    second = SecondAlter(\"gale_articles_Vol_3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10114217",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    third = SecondAlter(\"gale_articles_Vol_4.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cd0506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e4a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1c8c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    fourth = SecondAlter(\"gale_articles_Vol_5.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a38745b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    fifth = SecondAlter(\"gale_articles_Vol_1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4138f128",
   "metadata": {},
   "source": [
    "### <B>Cleaning Articles</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57d0cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def CleanArticle(article):\n",
    "    # Remove GALE/scan artifacts\n",
    "    cleaned_article = re.sub(r\"GALE ENCYCLOPEDIA OF MEDICINE.*?Page \\d+\", \"\", article)\n",
    "    cleaned_article = re.sub(r\"GEM\\s*-\\s*\\d+\\s*to\\s*\\d+\\s*-\\s*[A-Z]?\", \"\", cleaned_article)\n",
    "    cleaned_article = re.sub(r\"Page \\d+\", \"\", cleaned_article)\n",
    "    #Fix line breaks inside paragraphs\n",
    "    cleaned_article = re.sub(r\"(?<!\\n)(?<!\\n\\n)\\n(?![\\nâ€¢A-Z])\", \" \", cleaned_article )\n",
    "    #Normalize multiple newlines\n",
    "    cleaned_article = re.sub(r\"\\n{2,}\", \"\\n\\n\", cleaned_article)\n",
    "    #Strip excessive whitespace\n",
    "    cleaned_article = re.sub(r\"[ \\t]+\", \" \", cleaned_article).strip()\n",
    "    cleaned_article = re.sub(r\"(?:\\n)?Resources\\s*\\n(?:.*\\n)*\", \" \", cleaned_article, flags=re.IGNORECASE)\n",
    "    #Ensure section headers are separated clearly\n",
    "    cleaned_article = re.sub(r\"\\n([A-Z][a-z]+(?: [A-Z][a-z]+)*):?\\n\", r\"\\n\\n\\1\\n\\n\", cleaned_article)\n",
    "\n",
    "    return cleaned_article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80f730",
   "metadata": {},
   "source": [
    "### <b>Create Parent-Child chunking </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7c3f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "ParentDocs = {}\n",
    "def ParentChildChunking(cleaned_article,article_title):\n",
    "    chunks_doc = []\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=150,chunk_overlap= 50)\n",
    "    parent_id = str(uuid4())\n",
    "    ParentDocs[parent_id] = cleaned_article\n",
    "    # Chunk the article \n",
    "    chunks = text_splitter.create_documents(\n",
    "        [cleaned_article],\n",
    "        metadatas=[{\n",
    "            \"title\":article_title,# must be modified\n",
    "            \"parent_id\": parent_id}]\n",
    "    )\n",
    "    chunks_doc.extend(chunks)\n",
    "    # embedd the vectors \n",
    "    vectors = embedding_model.embed_documents([c.page_content for c in chunks_doc])\n",
    "    # Create Point for Qdrant injection \n",
    "    points = []\n",
    "\n",
    "    for index, doc in enumerate(chunks_doc):\n",
    "        point = PointStruct(\n",
    "            id=str(uuid4()),\n",
    "            vector=vectors[index],\n",
    "            payload={\n",
    "                **doc.metadata\n",
    "            }\n",
    "        )\n",
    "        points.append(point)\n",
    "     # Insert Points in Qdrant collection \n",
    "    qdrant.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84983f5c",
   "metadata": {},
   "source": [
    "## Working with the retrieval "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6737779",
   "metadata": {},
   "source": [
    "### Create the prompt for getting multiple queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "552a4154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output ( 3 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf083c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a9f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27434bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever_per_query(query:str,client):\n",
    "    query_vector = embedding_model.embed_query(query)\n",
    "    search_results = client.search(\n",
    "        collection_name= collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=5\n",
    "    )\n",
    "    return search_results\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
